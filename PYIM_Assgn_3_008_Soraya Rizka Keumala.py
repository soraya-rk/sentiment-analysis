# -*- coding: utf-8 -*-
"""Copy of Assignment 3 - Sentiment Analysis alt2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1j-Me2YpdpDv0UBCeoRTpHgIhTPWsHNeE

**Tujuan**: Memprediksi apakah sebuat teks review product memiliki sentimen positif atau negatif.
Data set yang digunakan:  Amazon product review dataset untuk produk kategori Baby (https://s3.amazonaws.com/amazon-reviews-pds/tsv/index.txt)

Algoritma yang digunakan adalah gensim dan Word2Vec.

# Preparation (Library Set Up)
"""

!pip install gensim --upgrade
!pip install keras --upgrade
!pip install pandas --upgrade

"""Lalu import library yang dibutuhkan."""

# Commented out IPython magic to ensure Python compatibility.
# DataFrame
import pandas as pd

# Matplot
import matplotlib.pyplot as plt
# %matplotlib inline

# Scikit-learn
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from sklearn.manifold import TSNE
from sklearn.feature_extraction.text import TfidfVectorizer

# Keras
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM
from keras import utils
from keras.callbacks import ReduceLROnPlateau, EarlyStopping

# nltk
import nltk
from nltk.corpus import stopwords
from  nltk.stem import SnowballStemmer

# Word2vec
import gensim

# Utility
import re
import numpy as np
import os
from collections import Counter
import logging
import time
import pickle
import itertools
import gzip

# Set log
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

"""***STOPWORDS***

Stop words adalah kata-kata yang sering dipakai namun tidak berguna dalam text processing, seperti 'yes, no, and the' dll. Library NLTK sudah menyediakan list stopwords dan dapat kita download.
"""

nltk.download('stopwords')

"""Predefine value."""

# DATASET
DATASET_COLUMNS = ["target", "ids", "date", "flag", "user", "text"]
DATASET_ENCODING = "ISO-8859-1"
TRAIN_SIZE = 0.8

# TEXT CLEANING
TEXT_CLEANING_RE = "@\S+|https?:\S+|http?:\S|[^A-Za-z0-9]+"

# WORD2VEC 
W2V_SIZE = 300
W2V_WINDOW = 7
W2V_EPOCH = 2   
W2V_MIN_COUNT = 10

# KERAS
SEQUENCE_LENGTH = 300
EPOCHS = 8
BATCH_SIZE = 1024

# EXPORT
KERAS_MODEL = "model.h5"
WORD2VEC_MODEL = "model.w2v"
TOKENIZER_MODEL = "tokenizer.pkl"
ENCODER_MODEL = "encoder.pkl"

"""# Data Preparation and Cleansing

Data sudah langsung didownload melalui website Amazon dan berbentuk .gz, jadi harus diunzip terlebih dahulu.
"""

# Download data
!curl -O https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Baby_v1_00.tsv.gz

# Unzip Data
!gunzip amazon_reviews_us_Baby_v1_00.tsv.gz

"""Data yang akan digunakan masih dalam format TSV (Tab-separated values). Karena dipisahkan oleh tab, data TSV sering kali ditemukan *badlines*, yaitu memiliki jumlah kolom yang tidak konsisten. Di data kita, kebanyakan data memiliki 15 kolom, namun ada sebagian kecil data yang terbagi dalam 22 kolom. Sebelum membuat dataframe, kita akan menghapus data yang memiliki 22 kolom."""

df = pd.read_csv('amazon_reviews_us_Baby_v1_00.tsv', sep='\t', error_bad_lines=False)

df.head()

"""Dari cuplikan tabel di atas, text yang ingin kita analisis di kolom *review body*. Sedangkan kolom yang menentukan tingkat sentimen dari review adalah *star_rating*, namun kolom *star_rating* masih dalam range 1, 2, 3, 4, dan 5.

Dalam klasifikasi ini, kita akan menggolongkan sebuah review ke dalam 2 label kategori: POSITIF (1), DAN NEGATIF(0). Terlebih dahulu kita akan mengkonversi *star_rating* ke dalam 2 kategori tersebut.

Untuk mempermudah training dan penilaian, kolom yang memiliki *star_review* = 3 akan kita eliminasikan.
"""

df = df[df['star_rating'] != 3]

# Membuat kondisi untuk mengisi kolom 'label' berdasarkan kolom 'star_rating'

conditions = [
    (df['star_rating'] == 1),
    (df['star_rating'] == 2),
  #  (df['star_rating'] == 3),
    (df['star_rating'] == 4),
    (df['star_rating'] == 5)
    ]

# List of values untuk kolom label
values = [0, 0, 1, 1]
# values = [-1, -1, 0, 1, 1]

# Buat kolom 'label' lalu isi sesuai conditions dan values yang sudah ditetapkan
df['label'] = np.select(conditions, values)

# Menampilkan data frame dengan kolom 'label'
df.tail()

label_cnt = Counter(df.label)

plt.figure(figsize=(16,8))
plt.bar(label_cnt.keys(), label_cnt.values())
plt.title("Dataset labels distribuition")

"""Buat data frame baru yang hanya berisikan *label* dan *review_body* saja."""

df2 = df[['label', 'review_body']].copy()
df2.head()

"""Buat kolom baru yaitu *text* yang berisikan copy dari *review_body* untuk  kita lakukan text processing. Kolom *text* harus berupa object/string."""

df2['text'] = df2["review_body"].astype(str)

df2.head()

"""Periksa data type untuk masing-masing kolom."""

df2.dtypes

"""# Text Processing

Sebelum masih ke training dan pembuatan model, ada beberapa tahap text processing yang harus dilalui, seperti lower-casing, words stemming (contoh, running --> run), penghapusan stop words, dan penghapusan tanda baca,

Stopwords package sudah didownload dari NLTK library.
"""

stop_words = stopwords.words("english")
stemmer = SnowballStemmer("english")

# Penghapusan tanda baca & stop word

def preprocess(text, stem=False):
    text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()
    tokens = []
    for token in text.split():
        if token not in stop_words:
            if stem:
                tokens.append(stemmer.stem(token))
            else:
                tokens.append(token)
    return " ".join(tokens)

# lower-casing

df2.text = df2.text.apply(lambda x: preprocess(x))

"""# Split Train and Test Data"""

df_train = df2.sample(frac=0.8, random_state=0)
df_test = df2.sample(frac=0.2, random_state=0)

"""# Word2Vec

Word2V
"""

documents = [_text.split() for _text in df_train.text]

w2v_model = gensim.models.word2vec.Word2Vec(window=W2V_WINDOW, 
                                            min_count=W2V_MIN_COUNT, 
                                            workers=8)

w2v_model.build_vocab(documents)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# w2v_model.train(documents, total_examples=len(documents), epochs=20)   #W2V_EPOCH

"""# Tokenizing"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# tokenizer = Tokenizer()
# tokenizer.fit_on_texts(df_train.text)
# 
# vocab_size = len(tokenizer.word_index) + 1
# print("Total words", vocab_size)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# x_train = pad_sequences(tokenizer.texts_to_sequences(df_train.text), maxlen=SEQUENCE_LENGTH)
# x_test = pad_sequences(tokenizer.texts_to_sequences(df_test.text), maxlen=SEQUENCE_LENGTH)

labels = df_train.label.unique().tolist()
labels

encoder = LabelEncoder()
encoder.fit(df_train.label.tolist())

y_train = encoder.transform(df_train.label.tolist())
y_test = encoder.transform(df_test.label.tolist())

y_train = y_train.reshape(-1,1)
y_test = y_test.reshape(-1,1)


print("x_train", x_train.shape)
print("y_train", y_train.shape)
print()
print("x_test", x_test.shape)
print("y_test", y_test.shape)



"""# Embedding"""

embedding_matrix = np.zeros((vocab_size, 100))
for word, i in tokenizer.word_index.items():
  if word in w2v_model.wv:
    embedding_matrix[i] = w2v_model.wv[word]
print(embedding_matrix.shape)

embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=SEQUENCE_LENGTH, trainable=False)

"""# Membuat model"""

model = Sequential()
model.add(embedding_layer)
model.add(Dropout(0.5))
model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(1, activation='sigmoid'))

model.summary()

"""## Compile Model"""

model.compile(loss='binary_crossentropy',
              optimizer="adam",
              metrics=['accuracy'])

callbacks = [ ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),
              EarlyStopping(monitor='val_acc', min_delta=1e-4, patience=5)]

# Commented out IPython magic to ensure Python compatibility.
# %%time
# history = model.fit(x_train, y_train,
#                     batch_size=BATCH_SIZE,
#                     epochs=1,
#                     validation_split=0.1,
#                     verbose=1,
#                     callbacks=callbacks)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# score = model.evaluate(x_test, y_test, batch_size=BATCH_SIZE)
# print()
# print("ACCURACY:",score[1])
# print("LOSS:",score[0])

"""# Predict Model

Model akan menghitung sentimen teks review dari dalam range 0 s/d 1. 0 = Paling negatif, dan 1 = paling positif. Kita atur jika nilai sentimen > 0.5, berarti sudah termasuk positif.
"""

# SENTIMENT
POSITIVE = "POSITIVE"
NEGATIVE = "NEGATIVE"
NEUTRAL = "NEUTRAL"
SENTIMENT_THRESHOLDS = (0, 0.5, 1)

def decode_sentiment(score, include_neutral=True):
    if include_neutral:        
        label = NEUTRAL
        if score <= SENTIMENT_THRESHOLDS[1]:
            label = NEGATIVE
        elif score >= SENTIMENT_THRESHOLDS[1]:
            label = POSITIVE

        return label
    else:
        return NEGATIVE if score < 0.5 else POSITIVE

"""Terlebih dahulu kita buat fungsi *predict()*."""

def predict(text, include_neutral=True):
    start_at = time.time()
    # Tokenize text
    x_test = pad_sequences(tokenizer.texts_to_sequences([text]), maxlen=SEQUENCE_LENGTH)
    # Predict
    score = model.predict([x_test])[0]
    # Decode sentiment
    label = decode_sentiment(score, include_neutral=include_neutral)

    return {"label": label, "score": float(score),
       "elapsed_time": time.time()-start_at}

"""Lalu coba input kalimat untuk memprediksi apakah kalimat review tersebut POSITIVE/NEGATIVE."""

# Prediksi
predict("The material used is low-quality. It breaks easily and color damped as it being washed. Overal, not a goood choice.")

predict("My son loves it so much. Definitely re-purchase.")

"""Untuk teks yang lebih panjang:"""

predict("Good way to keep a child's medical history in a single book: I bought this book for each of my children in order to keep their medical history in a single place. I take it with me to doctor's visits and write a synopsis about each illness, etc. I was better about it when they were babies, but try to keep it up to date now as well. Good to keep you organized.")

predict("Not essential at all: The authors try hard but end up writing what is unintentionally hilarious. Actually has sections like what the properly dressed leather man wears. Meant to be helpful, it ends up being immediatedly dated and campy.")